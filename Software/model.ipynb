{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897d8acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import joblib # Import joblib for saving/loading scaler and label_encoder\n",
    "import os # For creating directories and managing files\n",
    "\n",
    "# --- Configuration ---\n",
    "SENSOR_HZ = 67\n",
    "TIME_STEPS = int(SENSOR_HZ * 0.75)\n",
    "if TIME_STEPS == 0:\n",
    "    TIME_STEPS = 1\n",
    "print(f\"Calculated TIME_STEPS: {TIME_STEPS}\")\n",
    "\n",
    "N_FEATURES = 6 # accel_x, accel_y, accel_z, gyro_x, gyro_y, gyro_z\n",
    "\n",
    "# --- Define paths for data and models ---\n",
    "# It's good practice to define these relative to the script\n",
    "DATA_DIR = 'data'\n",
    "MODELS_DIR = 'models'\n",
    "CSV_FILE_NAME = 'gesture_datas1.csv'\n",
    "CSV_FILE_PATH = os.path.join(DATA_DIR, CSV_FILE_NAME)\n",
    "\n",
    "MODEL_H5_PATH = os.path.join(MODELS_DIR, 'simple_gesture_recognition_model.h5')\n",
    "LABEL_ENCODER_PATH = os.path.join(MODELS_DIR, 'simple_label_encoder.pkl')\n",
    "SCALER_PATH = os.path.join(MODELS_DIR, 'simple_scaler.pkl')\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# --- 1. Data Preprocessing (Offline - for your CSV) ---\n",
    "\n",
    "def load_and_preprocess_data(filepath, time_steps, n_features):\n",
    "    print(f\"Loading data from: {filepath}\")\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Error: Data file not found at {filepath}\")\n",
    "        print(\"Please ensure 'gesture_datas1.csv' is in the 'data/' directory.\")\n",
    "        raise FileNotFoundError(f\"CSV file not found: {filepath}\")\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    sensor_features = ['accel_x', 'accel_y', 'accel_z', 'gyro_x', 'gyro_y', 'gyro_z']\n",
    "    \n",
    "    if not all(feature in df.columns for feature in sensor_features):\n",
    "        print(f\"Error: Missing one or more expected sensor columns. Expected: {sensor_features}\")\n",
    "        raise ValueError(\"Missing sensor data columns in CSV.\")\n",
    "    if 'gesture_label' not in df.columns:\n",
    "        print(\"Error: Missing 'gesture_label' column in CSV.\")\n",
    "        raise ValueError(\"Missing 'gesture_label' column in CSV.\")\n",
    "\n",
    "    X = df[sensor_features].values\n",
    "    y = df['gesture_label'].values # Assuming you have a 'gesture_label' column\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    print(f\"Detected classes: {label_encoder.classes_}\")\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    # Using a stride of 1 to create overlapping sequences for more training data\n",
    "    for i in range(0, len(X_scaled) - time_steps + 1, 1):\n",
    "        sequence = X_scaled[i:i + time_steps]\n",
    "        # Label the sequence based on the last data point's label\n",
    "        labels.append(y_encoded[i + time_steps - 1]) \n",
    "        sequences.append(sequence)\n",
    "\n",
    "    X_seq = np.array(sequences)\n",
    "    y_seq = np.array(labels)\n",
    "\n",
    "    y_one_hot = to_categorical(y_seq, num_classes=num_classes)\n",
    "\n",
    "    print(f\"Shape of X_seq (sequences): {X_seq.shape}\")\n",
    "    print(f\"Shape of y_one_hot (one-hot labels): {y_one_hot.shape}\")\n",
    "    return X_seq, y_one_hot, label_encoder, scaler, num_classes\n",
    "\n",
    "# --- 2. Build a Simpler Neural Network Model ---\n",
    "\n",
    "def build_simple_model(time_steps, n_features, num_classes):\n",
    "    model = Sequential([\n",
    "        # LSTM layer processes sequences. return_sequences=False means output is a single vector per sequence.\n",
    "        LSTM(units=64, input_shape=(time_steps, n_features), return_sequences=False),\n",
    "        Dropout(0.3),\n",
    "        # Dense layers for classification\n",
    "        Dense(units=32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        # Output layer with softmax for multi-class classification\n",
    "        Dense(units=num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# --- 3. Training the Model ---\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, epochs=50, batch_size=128): # Increased epochs slightly\n",
    "    print(\"Training model...\")\n",
    "    # Early stopping to prevent overfitting and save best weights\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True) # Increased patience\n",
    "    \n",
    "    history = model.fit(X_train, y_train, \n",
    "                        epochs=epochs, \n",
    "                        batch_size=batch_size, \n",
    "                        validation_data=(X_val, y_val),\n",
    "                        callbacks=[early_stopping],\n",
    "                        verbose=1) # Set verbose to 1 to see progress during training\n",
    "    print(\"Training complete.\")\n",
    "    return history\n",
    "\n",
    "# --- Function to generate requirements.txt ---\n",
    "def generate_requirements_txt():\n",
    "    # This will capture the exact versions of installed packages\n",
    "    # It's better to run 'pip freeze > requirements.txt' in your environment\n",
    "    # but for a quick script-based generation, we can list common ones.\n",
    "    \n",
    "    # You should manually verify this against your 'pip freeze' output\n",
    "    # to ensure exact versions are captured for reproducibility.\n",
    "    \n",
    "    requirements_content = \"\"\"\n",
    "pandas\n",
    "numpy\n",
    "tensorflow\n",
    "scikit-learn\n",
    "joblib\n",
    "\"\"\"\n",
    "    with open('requirements.txt', 'w') as f:\n",
    "        f.write(requirements_content.strip())\n",
    "    print(\"Generated requirements.txt\")\n",
    "\n",
    "# --- Main execution for training ---\n",
    "if __name__ == '__main__':\n",
    "    # Generate requirements.txt first\n",
    "    generate_requirements_txt()\n",
    "\n",
    "    try:\n",
    "        X_seq, y_one_hot, label_encoder, scaler, NUM_CLASSES = load_and_preprocess_data(CSV_FILE_PATH, TIME_STEPS, N_FEATURES)\n",
    "    except (ValueError, FileNotFoundError) as e:\n",
    "        print(f\"Data loading failed: {e}\")\n",
    "        print(\"Please ensure your CSV file is correctly formatted and present in the 'data/' directory.\")\n",
    "        exit()\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_seq, y_one_hot, test_size=0.2, random_state=256, stratify=y_one_hot)\n",
    "\n",
    "    # Build the simpler model\n",
    "    model = build_simple_model(TIME_STEPS, N_FEATURES, NUM_CLASSES)\n",
    "    model.summary()\n",
    "\n",
    "    # Train the model\n",
    "    history = train_model(model, X_train, y_train, X_val, y_val)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    loss, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "    print(f\"\\n--- Model Evaluation ---\")\n",
    "    print(f\"Validation Loss: {loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Save the trained model, label encoder, and scaler for live prediction\n",
    "    model.save(MODEL_H5_PATH)\n",
    "    joblib.dump(label_encoder, LABEL_ENCODER_PATH)\n",
    "    joblib.dump(scaler, SCALER_PATH)\n",
    "    print(f\"\\nModel saved to: {MODEL_H5_PATH}\")\n",
    "    print(f\"Label Encoder saved to: {LABEL_ENCODER_PATH}\")\n",
    "    print(f\"Scaler saved to: {SCALER_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
